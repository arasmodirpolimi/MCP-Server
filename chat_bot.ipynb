{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7aaa823-c216-4d4f-837e-a88781e90912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mcp in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (1.15.0)\n",
      "Requirement already satisfied: jupyter-server-proxy in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: openai in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (1.1.1)\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: ollama in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.5.4)\n",
      "Requirement already satisfied: requests in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: anyio>=4.5 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (4.11.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (0.4.1)\n",
      "Requirement already satisfied: httpx>=0.27.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (4.25.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (2.11.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.11.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (2.11.9)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (0.0.20)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (3.0.2)\n",
      "Requirement already satisfied: starlette>=0.27 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (0.48.0)\n",
      "Requirement already satisfied: uvicorn>=0.31.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp) (0.37.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server-proxy) (3.13.0)\n",
      "Requirement already satisfied: jupyter-server>=1.24.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server-proxy) (2.17.0)\n",
      "Requirement already satisfied: simpervisor>=1.0.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server-proxy) (1.0.0)\n",
      "Requirement already satisfied: tornado>=6.1.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server-proxy) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server-proxy) (5.14.3)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from openai) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpx>=0.27.1->mcp) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpcore==1.*->httpx>=0.27.1->mcp) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp) (0.27.1)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (25.1.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (3.1.6)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (5.8.1)\n",
      "Requirement already satisfied: jupyter-events>=0.11.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.12.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.5.3)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (7.16.6)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (7.7.0)\n",
      "Requirement already satisfied: packaging>=22.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (25.0)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.23.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (3.0.2)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (27.1.0)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (1.8.3)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.18.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.11.0->mcp) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.11.0->mcp) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.11.0->mcp) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from uvicorn>=0.31.1->mcp) (8.3.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.22.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (25.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server>=1.24.0->jupyter-server-proxy) (3.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy) (4.4.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (4.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (6.0.3)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (4.14.2)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (6.2.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.3.0)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (3.1.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.10.2)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (1.5.1)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.19.2)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server>=1.24.0->jupyter-server-proxy) (2.21.2)\n",
      "Requirement already satisfied: webencodings in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from bleach!=5.0.0->bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.5.1)\n",
      "Requirement already satisfied: tinycss2<1.5,>=1.1.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from bleach[css]!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (1.4.0)\n",
      "Requirement already satisfied: fqdn in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (3.0.0)\n",
      "Requirement already satisfied: rfc3987-syntax>=1.1.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.1.0)\n",
      "Requirement already satisfied: uri-template in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (24.11.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=7.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (1.17.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (2.0.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.8)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (2.23)\n",
      "Requirement already satisfied: lark>=1.2.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from rfc3987-syntax>=1.1.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.3.0)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.3.0)\n",
      "Requirement already satisfied: types-python-dateutil>=2.8.10 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from arrow>=0.15.0->isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.11.0->jupyter-server>=1.24.0->jupyter-server-proxy) (2.9.0.20251008)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.3.27)\n",
      "Requirement already satisfied: langchain-core in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.3.78)\n",
      "Requirement already satisfied: langchain-community in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.3.31)\n",
      "Requirement already satisfied: langchain-ollama in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.3.10)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain) (0.3.11)\n",
      "Requirement already satisfied: langsmith>=0.1.17 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain) (0.4.33)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain) (2.11.9)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain) (2.0.43)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain) (2.32.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-community) (3.13.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-community) (2.11.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-community) (0.4.1)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-community) (2.3.3)\n",
      "Requirement already satisfied: ollama<1.0.0,>=0.5.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langchain-ollama) (0.5.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.1.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
      "Requirement already satisfied: anyio in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (4.57.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.2.1)\n",
      "Requirement already satisfied: torch in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (2.8.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (2.3.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (2025.9.18)\n",
      "Requirement already satisfied: requests in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from torch) (2025.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from requests->transformers) (2025.8.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ollmcp in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.18.2)\n",
      "Requirement already satisfied: fastapi in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.118.3)\n",
      "Requirement already satisfied: uvicorn in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.37.0)\n",
      "Requirement already satisfied: httpx in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (0.28.1)\n",
      "Requirement already satisfied: pydantic in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (2.11.9)\n",
      "Requirement already satisfied: mcp-client-for-ollama==0.18.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from ollmcp) (0.18.2)\n",
      "Collecting mcp~=1.12.4 (from mcp-client-for-ollama==0.18.2->ollmcp)\n",
      "  Using cached mcp-1.12.4-py3-none-any.whl.metadata (68 kB)\n",
      "Requirement already satisfied: ollama~=0.5.3 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp-client-for-ollama==0.18.2->ollmcp) (0.5.4)\n",
      "Requirement already satisfied: prompt-toolkit~=3.0.51 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp-client-for-ollama==0.18.2->ollmcp) (3.0.52)\n",
      "Requirement already satisfied: rich~=14.1.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp-client-for-ollama==0.18.2->ollmcp) (14.1.0)\n",
      "Requirement already satisfied: typer~=0.16.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp-client-for-ollama==0.18.2->ollmcp) (0.16.1)\n",
      "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from fastapi) (0.48.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from fastapi) (4.15.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from uvicorn) (8.3.0)\n",
      "Requirement already satisfied: h11>=0.8 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from uvicorn) (0.16.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpx) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpx) (2025.8.3)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpx) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from httpx) (3.10)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from click>=7.0->uvicorn) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from anyio->httpx) (1.3.1)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (0.4.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (4.25.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (2.11.0)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (0.0.20)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (3.0.2)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from prompt-toolkit~=3.0.51->mcp-client-for-ollama==0.18.2->ollmcp) (0.2.14)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from rich~=14.1.0->mcp-client-for-ollama==0.18.2->ollmcp) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from rich~=14.1.0->mcp-client-for-ollama==0.18.2->ollmcp) (2.19.2)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from typer~=0.16.0->mcp-client-for-ollama==0.18.2->ollmcp) (1.5.4)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from jsonschema>=4.20.0->mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (0.27.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich~=14.1.0->mcp-client-for-ollama==0.18.2->ollmcp) (0.1.2)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\arsalan\\desktop\\mcp_example\\.venv\\lib\\site-packages (from pydantic-settings>=2.5.2->mcp~=1.12.4->mcp-client-for-ollama==0.18.2->ollmcp) (1.1.1)\n",
      "Using cached mcp-1.12.4-py3-none-any.whl (160 kB)\n",
      "Installing collected packages: mcp\n",
      "  Attempting uninstall: mcp\n",
      "    Found existing installation: mcp 1.15.0\n",
      "    Uninstalling mcp-1.15.0:\n",
      "      Successfully uninstalled mcp-1.15.0\n",
      "Successfully installed mcp-1.12.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install mcp jupyter-server-proxy openai python-dotenv nest_asyncio ollama requests\n",
    "%pip install langchain langchain-core langchain-community langchain-ollama\n",
    "%pip install -U transformers sentencepiece torch\n",
    "%pip install ollmcp fastapi uvicorn httpx pydantic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96af0c",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Create a .env file in project root with:\n",
    "\n",
    "OPENWEATHER_API_KEY=YOUR_KEY_HERE\n",
    "OPENWEATHER_BASE_URL=https://api.openweathermap.org/data/2.5/weather\n",
    "OPENAI_API_KEY=YOUR_OPENAI_KEY\n",
    "\n",
    "If only OPEN_AI_API_KEY exists it will be aliased automatically.\n",
    "Restart the kernel after changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a33bbe5-cc79-4e8a-ba37-4633a71a6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load variables from .env if present\n",
    "\n",
    "def ensure_env(name: str, default: str | None = None, prompt: bool = True, secret: bool = False) -> str:\n",
    "    val = os.getenv(name)\n",
    "    if val:\n",
    "        return val\n",
    "    if default is not None:\n",
    "        os.environ[name] = default\n",
    "        return default\n",
    "    if prompt:\n",
    "        try:\n",
    "            entered = input(f\"Enter value for {name}: \").strip()\n",
    "            if entered:\n",
    "                os.environ[name] = entered\n",
    "                return entered\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(f\"{name} is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53d64e15-5aa2-4834-82b6-c7ad2fbf5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str, unit: str = \"celsius\") -> Dict[str, Any]:\n",
    "    \"\"\"Fetch current weather from OpenWeather.\"\"\"\n",
    "    key = ensure_env(\"OPENWEATHER_API_KEY\", prompt=True)\n",
    "    base_url = os.getenv(\"OPENWEATHER_BASE_URL\") or \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    unit_map = {\"celsius\": \"metric\", \"fahrenheit\": \"imperial\"}\n",
    "    owm_unit = unit_map.get(unit.lower(), \"metric\")\n",
    "    params = {\"q\": location, \"units\": owm_unit, \"appid\": key}\n",
    "    resp = requests.get(base_url, params=params, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    resolved_name = data.get(\"name\") or location\n",
    "    temp = (data.get(\"main\") or {}).get(\"temp\")\n",
    "    weather_list = data.get(\"weather\")\n",
    "    forecast = [w.get(\"description\") for w in weather_list if isinstance(w, dict) and w.get(\"description\")] if isinstance(weather_list, list) else []\n",
    "    return {\n",
    "        \"location\": resolved_name,\n",
    "        \"temperature\": temp,\n",
    "        \"unit\": \"celsius\" if owm_unit == \"metric\" else \"fahrenheit\",\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "# get_current_weather(\"Milan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9208723d-0851-4cc5-83ba-08b37eb0c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI tool schemas (JSON Schema)\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get current weather for a location using OpenWeather. Units can be 'celsius' or 'fahrenheit'.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"City name (optionally with country code)\"},\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"default\": \"celsius\",\n",
    "                        \"description\": \"Temperature unit\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c02a6245-f474-4549-acb3-8192304b7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tool names to Python callables\n",
    "mapping_tool_function = {\n",
    "    \"get_current_weather\": get_current_weather\n",
    "}\n",
    "\n",
    "def execute_tool(tool_name: str, tool_args: Dict[str, Any]) -> str:\n",
    "    result = mapping_tool_function[tool_name](**tool_args)\n",
    "    if result is None:\n",
    "        return \"The operation completed but didn't return any results.\"\n",
    "    if isinstance(result, list):\n",
    "        return \", \".join(map(str, result))\n",
    "    if isinstance(result, dict):\n",
    "        return json.dumps(result, indent=2)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb3e00ae-6aa9-46cb-9dec-11f8320f5e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI model: gpt-3.5-turbo\n",
      "The current weather in New York is 9.43°C with clear skies, and in Milan, it is 21.44°C with scattered clouds.\n"
     ]
    }
   ],
   "source": [
    "# ----------------- OpenAI chat with tool calling -----------------\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "BASE_OPENAI_URL = os.getenv(\"BASE_OPENAI_URL\", \"https://api.openai.com/v1\")\n",
    "client = OpenAI(api_key=api_key, base_url=BASE_OPENAI_URL)\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "print(f\"Using OpenAI model: {OPENAI_MODEL}\")\n",
    "\n",
    "def process_query(query: str) -> None:\n",
    "    messages: List[Dict[str, Any]] = [{\"role\": \"user\", \"content\": query}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        temperature=0.01\n",
    "    )\n",
    "    while True:\n",
    "        choice = response.choices[0]\n",
    "        msg = choice.message\n",
    "        if msg.tool_calls:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": msg.content or \"\",\n",
    "                \"tool_calls\": [tc.model_dump() for tc in msg.tool_calls]\n",
    "            })\n",
    "            for tc in msg.tool_calls:\n",
    "                name = tc.function.name\n",
    "                args = json.loads(tc.function.arguments or \"{}\")\n",
    "                result = execute_tool(name, args)\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tc.id,\n",
    "                    \"content\": result\n",
    "                })\n",
    "            response = client.chat.completions.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "                tool_choice=\"auto\",\n",
    "                temperature=0.01\n",
    "            )\n",
    "            continue\n",
    "        if msg.content:\n",
    "            print(msg.content)\n",
    "        break\n",
    "    \n",
    "process_query(\"What's the weather like in New York and Milan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecd4f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"gpt-4-0613\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1686588896,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1687882411,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1677610602,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"sora-2-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759708663,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-mini-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759512137,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759517133,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-mini-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759517175,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"sora-2\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759708615,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"davinci-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692634301,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"babbage-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692634615,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692901427,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1694122472,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"dall-e-3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698785189,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"dall-e-2\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698798177,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-1106-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698957206,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698959748,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-hd\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699046015,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699053241,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-hd-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699053533,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-3-small\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1705948997,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-3-large\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1705953180,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-0125-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706037612,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706037777,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-0125\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706048358,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1712361441,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo-2024-04-09\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1712601677,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1715367049,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-05-13\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1715368132,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-2024-07-18\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1721172717,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1721172741,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-08-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1722814719,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"chatgpt-4o-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1723515131,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-mini-2024-09-12\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1725648979,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1725649008,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2024-10-01\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727131766,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2024-10-01\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727389042,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727460443,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727659998,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"omni-moderation-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1731689265,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"omni-moderation-2024-09-26\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1732734466,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1733945430,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734034239,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-realtime-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734112601,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-audio-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734115920,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734326976,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734375816,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-realtime-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734387380,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-audio-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734387424,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1737146383,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-mini-2025-01-31\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1738010200,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-11-20\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1739331543,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-search-preview-2025-03-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741388170,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-search-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741388720,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-search-preview-2025-03-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741390858,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-search-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741391161,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-transcribe\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742068463,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-transcribe\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742068596,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-pro-2025-03-19\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742251504,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742251791,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-tts\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742403959,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-2025-04-16\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744133301,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-2025-04-16\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744133506,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744225308,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744225351,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744315746,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744316542,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-mini-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744317547,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744318173,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-nano-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744321025,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-nano\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744321707,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-image-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1745517030,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"codex-mini-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1746673257,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2025-06-03\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748907838,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2025-06-03\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748908498,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-deep-research\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1749685485,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-deep-research-2025-06-26\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1750866121,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-chat-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754073306,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754075360,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425777,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-mini-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425867,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425928,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-nano-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754426303,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-nano\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754426384,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-2025-08-28\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756256146,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756271701,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-2025-08-28\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756271773,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756339249,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-codex\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1757527818,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-image-1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1758845821,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-pro-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759469707,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759469822,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759512027,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-16k\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1683758102,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1681940951,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"whisper-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1677532384,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-ada-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1671217299,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6CmnzQZ:ckpt-step-95\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042481,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6ChQMTs:ckpt-step-95\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042148,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6ChRTvE:ckpt-step-190\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042149,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6ChRu7M\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042149,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6H0pg4f:ckpt-step-95\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726058728,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6H0qVBU:ckpt-step-190\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726058728,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6H0qNVD\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726058728,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6CmoRkI:ckpt-step-190\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042482,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6CmoFtt\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042482,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "import os\n",
    "!curl -s https://api.openai.com/v1/models -H \"Authorization: Bearer {os.environ['OPENAI_API_KEY']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b1809b7-db8c-4779-aadd-c703083c01f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == \"quit\":\n",
    "                break\n",
    "            process_query(query)\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b02c22eb-60df-42c9-b3d2-8a179d007c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n",
      "Hello! How can I assist you today?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a79913c-aad4-4438-a103-751d011211f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_server.py\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "import os, json, requests\n",
    "from typing import Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "mcp = FastMCP(\"weather\")\n",
    "def ensure_env(name: str, default: str | None = None) -> str:\n",
    "    val = os.getenv(name)\n",
    "    if val:\n",
    "        return val\n",
    "    if default is not None:\n",
    "        os.environ[name] = default\n",
    "        return default\n",
    "    raise RuntimeError(f\"{name} is not set\")\n",
    "@mcp.tool()\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> Dict[str, Any]:\n",
    "    key = ensure_env(\"OPENWEATHER_API_KEY\")\n",
    "    base_url = os.getenv(\"OPENWEATHER_BASE_URL\") or \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    unit_map = {\"celsius\": \"metric\", \"fahrenheit\": \"imperial\"}\n",
    "    owm_unit = unit_map.get(unit.lower(), \"metric\")\n",
    "    params = {\"q\": location, \"units\": owm_unit, \"appid\": key}\n",
    "    resp = requests.get(base_url, params=params, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    resolved_name = data.get(\"name\") or location\n",
    "    temp = (data.get(\"main\") or {}).get(\"temp\")\n",
    "    weather_list = data.get(\"weather\")\n",
    "    forecast = [w.get(\"description\") for w in weather_list if isinstance(w, dict) and w.get(\"description\")] if isinstance(weather_list, list) else []\n",
    "    return {\n",
    "        \"location\": resolved_name,\n",
    "        \"temperature\": temp,\n",
    "        \"unit\": \"celsius\" if owm_unit == \"metric\" else \"fahrenheit\",\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport='stdio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd33fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_chatbot_openai.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_chatbot_openai.py\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, Any\n",
    "import asyncio, json, os, nest_asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def _flatten_tool_content(content_list):\n",
    "    parts = []\n",
    "    for item in content_list:\n",
    "        # TextContent (MCP) typically has .text\n",
    "        text = getattr(item, 'text', None)\n",
    "        if text is not None:\n",
    "            parts.append(text)\n",
    "        elif isinstance(item, dict):\n",
    "            parts.append(json.dumps(item, ensure_ascii=False))\n",
    "        else:\n",
    "            parts.append(str(item))\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        self.session: ClientSession | None = None\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.available_tools: List[dict] = []\n",
    "\n",
    "    def _openai_tools_from_mcp(self, mcp_tools: List[types.Tool]) -> List[dict]:\n",
    "        out = []\n",
    "        for t in mcp_tools:\n",
    "            out.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description or \"\",\n",
    "                    \"parameters\": t.inputSchema or {\"type\": \"object\", \"properties\": {}}\n",
    "                }\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    async def process_query(self, query: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        messages: List[Dict[str, Any]] = [{\"role\": \"user\", \"content\": query}]\n",
    "        while True:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=self.available_tools or None,\n",
    "                tool_choice=\"auto\" if self.available_tools else \"none\",\n",
    "                temperature=0.01\n",
    "            )\n",
    "            msg = response.choices[0].message\n",
    "            if msg.tool_calls:\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": msg.content or \"\",\n",
    "                    \"tool_calls\": [tc.model_dump() for tc in msg.tool_calls]\n",
    "                })\n",
    "                for tc in msg.tool_calls:\n",
    "                    tool_name = tc.function.name\n",
    "                    raw_args = tc.function.arguments\n",
    "                    try:\n",
    "                        args = json.loads(raw_args) if isinstance(raw_args, str) else (raw_args or {})\n",
    "                    except Exception:\n",
    "                        args = {}\n",
    "                    result = await self.session.call_tool(tool_name, arguments=args)\n",
    "                    flattened = _flatten_tool_content(result.content)\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tc.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": flattened\n",
    "                    })\n",
    "                continue\n",
    "            if msg.content:\n",
    "                print(msg.content.strip())\n",
    "            break\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        print(\"MCP Chatbot Started. Type your queries or 'quit'.\")\n",
    "        while True:\n",
    "            try:\n",
    "                q = input(\"\\nQuery: \").strip()\n",
    "                if q.lower() == 'quit':\n",
    "                    break\n",
    "                await self.process_query(q)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    async def connect_to_server_and_run(self):\n",
    "        server_params = StdioServerParameters(command=\"uv\", args=[\"run\", \"weather_server.py\"], env=None)\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                await session.initialize()\n",
    "                resp = await session.list_tools()\n",
    "                self.available_tools = self._openai_tools_from_mcp(resp.tools)\n",
    "                await self.chat_loop()\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53610722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_chatbot_openai.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_chatbot_openai.py\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict, Any, Union\n",
    "from contextlib import AsyncExitStack\n",
    "import asyncio, json, os\n",
    "\n",
    "# ===== LangChain imports (with fallbacks for version differences) =====\n",
    "try:\n",
    "    from langchain_openai import ChatOpenAI  # >= 0.2 style\n",
    "except Exception:\n",
    "    from langchain.chat_models import ChatOpenAI  # legacy fallback\n",
    "\n",
    "try:\n",
    "    from langchain_community.chat_message_histories import FileChatMessageHistory\n",
    "except Exception:\n",
    "    # older versions\n",
    "    from langchain.memory.chat_message_histories import FileChatMessageHistory  # type: ignore\n",
    "\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def _flatten_tool_content(content_list) -> str:\n",
    "    \"\"\"Flatten MCP CallToolResult.content into plain text for OpenAI 'tool' message.\"\"\"\n",
    "    parts = []\n",
    "    if isinstance(content_list, list):\n",
    "        for item in content_list:\n",
    "            t = getattr(item, \"type\", None) or (isinstance(item, dict) and item.get(\"type\"))\n",
    "            if t == \"text\":\n",
    "                txt = getattr(item, \"text\", None) or (isinstance(item, dict) and item.get(\"text\"))\n",
    "                if txt is not None:\n",
    "                    parts.append(str(txt))\n",
    "            elif t in (\"json\", \"object\"):\n",
    "                data = getattr(item, \"data\", None) or (isinstance(item, dict) and (item.get(\"data\") or item.get(\"value\")))\n",
    "                try:\n",
    "                    parts.append(json.dumps(data, ensure_ascii=False))\n",
    "                except Exception:\n",
    "                    parts.append(str(data))\n",
    "            else:\n",
    "                try:\n",
    "                    parts.append(json.dumps(item, default=str, ensure_ascii=False))\n",
    "                except Exception:\n",
    "                    parts.append(str(item))\n",
    "    elif content_list is not None:\n",
    "        if isinstance(content_list, (dict, list)):\n",
    "            parts.append(json.dumps(content_list, ensure_ascii=False))\n",
    "        else:\n",
    "            parts.append(str(content_list))\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        # --- OpenAI client for actual chat/tool-calling loop ---\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "        # --- MCP infra ---\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.sessions: List[ClientSession] = []\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}\n",
    "        self.available_tools: List[ToolDefinition] = []  # OpenAI function-calling schema\n",
    "\n",
    "        # --- LangChain Memory (NEW) ---\n",
    "        # Persistent history file (JSON). Change path via LC_HISTORY_PATH env var if you like.\n",
    "        history_path = os.getenv(\"LC_HISTORY_PATH\", \".mcp_chat_history.json\")\n",
    "        self.history = FileChatMessageHistory(history_path)\n",
    "\n",
    "        # LLM used only for summarizing memory (cheap/small recommended)\n",
    "        summary_model = os.getenv(\"OPENAI_SUMMARY_MODEL\", \"gpt-3.5-turbo\")\n",
    "        self.summary_llm = ChatOpenAI(\n",
    "            model=summary_model,\n",
    "            temperature=0.0,\n",
    "            # LangChain uses OPENAI_API_KEY env under the hood\n",
    "        )\n",
    "\n",
    "        # ConversationSummaryBufferMemory will auto-summarize older turns.\n",
    "        # Increase/decrease max_token_limit based on how much context you want retained.\n",
    "        self.memory = ConversationSummaryBufferMemory(\n",
    "            llm=self.summary_llm,\n",
    "            chat_memory=self.history,\n",
    "            return_messages=True,           # we’ll convert messages to OpenAI format\n",
    "            input_key=\"input\",\n",
    "            output_key=\"output\",\n",
    "            max_token_limit=int(os.getenv(\"LC_MAX_TOKENS\", \"3000\")),\n",
    "        )\n",
    "\n",
    "    # ---------------------- MCP wiring ----------------------\n",
    "    def _openai_tools_from_mcp(self, mcp_tools: List[types.Tool]) -> List[dict]:\n",
    "        tools = []\n",
    "        for t in mcp_tools:\n",
    "            tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description or \"\",\n",
    "                    \"parameters\": t.inputSchema or {\"type\": \"object\", \"properties\": {}},\n",
    "                }\n",
    "            })\n",
    "        return tools\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_cfg: dict):\n",
    "        \"\"\"Connect to one MCP server (stdio).\"\"\"\n",
    "        try:\n",
    "            params = StdioServerParameters(**server_cfg)\n",
    "            read, write = await self.exit_stack.enter_async_context(stdio_client(params))\n",
    "            session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
    "            await session.initialize()\n",
    "\n",
    "            self.sessions.append(session)\n",
    "\n",
    "            resp = await session.list_tools()\n",
    "            tools = resp.tools or []\n",
    "            print(f\"Connected to {server_name} with tools:\", [t.name for t in tools])\n",
    "\n",
    "            for t in tools:\n",
    "                self.tool_to_session[t.name] = session\n",
    "            self.available_tools.extend(self._openai_tools_from_mcp(tools))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self, config_path: str = \"server_config.json\"):\n",
    "        \"\"\"Read server_config.json and connect to each server.\"\"\"\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "        servers = cfg.get(\"mcpServers\", {})\n",
    "        for name, server_cfg in servers.items():\n",
    "            await self.connect_to_server(name, server_cfg)\n",
    "\n",
    "    # ---------------------- Memory helpers (NEW) ----------------------\n",
    "    def _lc_history_to_openai_messages(self, history: Union[str, List[Any]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Convert LangChain memory 'history' to OpenAI Chat Completions messages.\"\"\"\n",
    "        msgs: List[Dict[str, str]] = []\n",
    "        if isinstance(history, str):\n",
    "            if history.strip():\n",
    "                msgs.append({\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": f\"Conversation summary so far:\\n{history}\"\n",
    "                })\n",
    "            return msgs\n",
    "\n",
    "        # history is a list of BaseMessage\n",
    "        for m in history:\n",
    "            if isinstance(m, HumanMessage):\n",
    "                msgs.append({\"role\": \"user\", \"content\": m.content})\n",
    "            elif isinstance(m, AIMessage):\n",
    "                # We store only the assistant's final content between turns.\n",
    "                msgs.append({\"role\": \"assistant\", \"content\": m.content})\n",
    "            elif isinstance(m, SystemMessage):\n",
    "                msgs.append({\"role\": \"system\", \"content\": m.content})\n",
    "            else:\n",
    "                # Skip tool/other message types to keep things simple\n",
    "                continue\n",
    "        return msgs\n",
    "\n",
    "    def _remember_turn(self, user_text: str, ai_text: str):\n",
    "        \"\"\"Append the latest user/assistant messages to LangChain memory.\"\"\"\n",
    "        self.memory.chat_memory.add_user_message(user_text)\n",
    "        self.memory.chat_memory.add_ai_message(ai_text)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        \"\"\"Erase persisted memory (type 'forget' in the REPL).\"\"\"\n",
    "        self.history.clear()\n",
    "        print(\"✅ Memory cleared.\")\n",
    "\n",
    "    # ---------------------- Chat loop with memory ----------------------\n",
    "    async def process_query(self, query: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        # 1) Pull prior memory (summarized or raw messages) and convert to OpenAI format\n",
    "        prior = self.memory.load_memory_variables({}).get(\"history\", [])\n",
    "        messages: List[Dict[str, Any]] = self._lc_history_to_openai_messages(prior)\n",
    "\n",
    "        # 2) Add the new user input\n",
    "        messages.append({\"role\": \"user\", \"content\": query})\n",
    "\n",
    "        # 3) Chat/tool loop\n",
    "        while True:\n",
    "            resp = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=self.available_tools or None,\n",
    "                tool_choice=\"auto\" if self.available_tools else \"none\",\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            msg = resp.choices[0].message\n",
    "\n",
    "            if msg.tool_calls:\n",
    "                # Keep assistant msg (with tool_calls) in history for the model context\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": msg.content or \"\",\n",
    "                    \"tool_calls\": [tc.model_dump() for tc in msg.tool_calls],\n",
    "                })\n",
    "\n",
    "                for tc in msg.tool_calls:\n",
    "                    tool_name = tc.function.name\n",
    "                    raw_args = tc.function.arguments\n",
    "                    try:\n",
    "                        args = json.loads(raw_args) if isinstance(raw_args, str) else (raw_args or {})\n",
    "                    except Exception:\n",
    "                        args = {}\n",
    "\n",
    "                    session = self.tool_to_session.get(tool_name)\n",
    "                    if not session:\n",
    "                        messages.append({\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tc.id,\n",
    "                            \"name\": tool_name,\n",
    "                            \"content\": f\"Tool '{tool_name}' is not available.\",\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    print(f\"Calling tool {tool_name} with args {args}\")\n",
    "                    result = await session.call_tool(tool_name, arguments=args)\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tc.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": _flatten_tool_content(result.content),\n",
    "                    })\n",
    "                continue  # let the model read tool results and continue\n",
    "\n",
    "            # Final answer (no tool calls)\n",
    "            final_text = (msg.content or \"\").strip()\n",
    "            if final_text:\n",
    "                print(final_text)\n",
    "\n",
    "            # 4) Store this turn in memory (NEW)\n",
    "            #    We store only user query and final assistant message for compactness.\n",
    "            try:\n",
    "                self._remember_turn(query, final_text if final_text else \"[no text]\")\n",
    "            except Exception as mem_err:\n",
    "                # Don’t crash chat if memory write fails\n",
    "                print(f\"(Memory error — continuing without memory this turn): {mem_err}\")\n",
    "\n",
    "            break\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        print(f\"\\nMCP Chatbot (OpenAI {os.getenv('OPENAI_SUMMARY_MODEL')} + LangChain Memory) — type your query, 'forget' to clear memory, or 'quit' to exit.\")\n",
    "        while True:\n",
    "            try:\n",
    "                q = input(\"\\nQuery: \").strip()\n",
    "                if q.lower() == \"quit\":\n",
    "                    break\n",
    "                if q.lower() == \"forget\":\n",
    "                    self.clear_memory()\n",
    "                    continue\n",
    "                await self.process_query(q)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    async def run(self):\n",
    "        try:\n",
    "            await self.connect_to_servers()\n",
    "            await self.chat_loop()\n",
    "        finally:\n",
    "            await self.exit_stack.aclose()\n",
    "\n",
    "    async def cleanup(self):  # existing\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        await chatbot.connect_to_servers()  # init MCP servers\n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1946ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict, Any, Optional, Union\n",
    "from contextlib import AsyncExitStack\n",
    "import asyncio, json, os, requests\n",
    "\n",
    "# ===== LangChain imports (local summarizer + persistent history) =====\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain.memory import ConversationSummaryBufferMemory   # <- use this\n",
    "from langchain_community.chat_message_histories import FileChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------------\n",
    "# Ollama adapter (no OpenAI)\n",
    "# ---------------------------\n",
    "class OllamaAdapter:\n",
    "    def __init__(self, base: str = \"http://127.0.0.1:11434\"):\n",
    "        self.base = base.rstrip(\"/\")\n",
    "\n",
    "    def chat_once(\n",
    "        self,\n",
    "        model: str,\n",
    "        messages: List[Dict[str, Any]],\n",
    "        temperature: float = 0.1,\n",
    "        tools: Optional[List[dict]] = None,\n",
    "        tool_choice: str | dict | None = \"auto\",\n",
    "    ):\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature},\n",
    "        }\n",
    "        if tools:\n",
    "            payload[\"tools\"] = tools\n",
    "        if tool_choice:\n",
    "            payload[\"tool_choice\"] = tool_choice\n",
    "\n",
    "        r = requests.post(f\"{self.base}/api/chat\", json=payload, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        msg = data.get(\"message\", {}) or {}\n",
    "        # Normalize into an OpenAI-like shape the rest of the code expects\n",
    "        return {\n",
    "            \"choices\": [{\n",
    "                \"message\": {\n",
    "                    \"role\": msg.get(\"role\", \"assistant\"),\n",
    "                    \"content\": msg.get(\"content\", \"\"),\n",
    "                    \"tool_calls\": msg.get(\"tool_calls\") or [],\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "\n",
    "def _flatten_tool_content(content_list) -> str:\n",
    "    parts = []\n",
    "    if isinstance(content_list, list):\n",
    "        for item in content_list:\n",
    "            t = getattr(item, \"type\", None) or (isinstance(item, dict) and item.get(\"type\"))\n",
    "            if t == \"text\":\n",
    "                txt = getattr(item, \"text\", None) or (isinstance(item, dict) and item.get(\"text\"))\n",
    "                if txt is not None:\n",
    "                    parts.append(str(txt))\n",
    "            elif t in (\"json\", \"object\"):\n",
    "                data = getattr(item, \"data\", None) or (isinstance(item, dict) and (item.get(\"data\") or item.get(\"value\")))\n",
    "                try:\n",
    "                    parts.append(json.dumps(data, ensure_ascii=False))\n",
    "                except Exception:\n",
    "                    parts.append(str(data))\n",
    "            else:\n",
    "                try:\n",
    "                    parts.append(json.dumps(item, default=str, ensure_ascii=False))\n",
    "                except Exception:\n",
    "                    parts.append(str(item))\n",
    "    elif content_list is not None:\n",
    "        if isinstance(content_list, (dict, list)):\n",
    "            parts.append(json.dumps(content_list, ensure_ascii=False))\n",
    "        else:\n",
    "            parts.append(str(content_list))\n",
    "    return \"\\n\".join(parts).strip()\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        # ===== Models & endpoints =====\n",
    "        self.model_default = os.getenv(\"LOCAL_MODEL\", \"qwen2.5:1.5b\")\n",
    "        self.client = OllamaAdapter(os.getenv(\"LOCAL_BASE_URL\", \"http://127.0.0.1:11434\"))\n",
    "\n",
    "        # ===== MCP infra =====\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.sessions: List[ClientSession] = []\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}\n",
    "        self.available_tools: List[ToolDefinition] = []\n",
    "\n",
    "        # ===== LangChain Memory (NEW) =====\n",
    "        # 1) Persistent chat history file (override with LC_HISTORY_PATH)\n",
    "        history_path = os.getenv(\"LC_HISTORY_PATH\", \".mcp_chat_history.json\")\n",
    "        self.history = FileChatMessageHistory(history_path)\n",
    "\n",
    "        # 2) Local summarizer model via Ollama (override with LC_SUMMARY_MODEL)\n",
    "        #    Pick a small/fast model installed in Ollama (e.g., llama3.2:3b, qwen2.5:1.5b, phi4:latest).\n",
    "        summary_model = os.getenv(\"LC_SUMMARY_MODEL\", \"qwen2.5:1.5b\")\n",
    "        self.summary_llm = ChatOllama(\n",
    "            model=summary_model,\n",
    "            base_url=os.getenv(\"LOCAL_BASE_URL\", \"http://127.0.0.1:11434\"),\n",
    "            temperature=0.0\n",
    "        )\n",
    "\n",
    "        # 3) Summary buffer to keep memory compact\n",
    "        self.memory = ConversationSummaryBufferMemory(\n",
    "            llm=self.summary_llm,\n",
    "            chat_memory=self.history,          \n",
    "            return_messages=True,\n",
    "            input_key=\"input\",\n",
    "            output_key=\"output\",\n",
    "            max_token_limit=int(os.getenv(\"LC_MAX_TOKENS\", \"3000\")),\n",
    "        )\n",
    "\n",
    "    # ---------------------- MCP wiring ----------------------\n",
    "    def _openai_tools_from_mcp(self, mcp_tools: List[types.Tool]) -> List[dict]:\n",
    "        out = []\n",
    "        for t in mcp_tools:\n",
    "            out.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description or \"\",\n",
    "                    \"parameters\": t.inputSchema or {\"type\": \"object\", \"properties\": {}},\n",
    "                }\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    async def _call_mcp_tool(self, tool_name: str, args: dict) -> Optional[str]:\n",
    "        session = self.tool_to_session.get(tool_name)\n",
    "        if not session:\n",
    "            return f\"Tool '{tool_name}' is not registered.\"\n",
    "        try:\n",
    "            result = await session.call_tool(tool_name, arguments=args or {})\n",
    "            return _flatten_tool_content(result.content) or \"(empty result)\"\n",
    "        except Exception as e:\n",
    "            return f\"Tool '{tool_name}' failed: {e}\"\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_cfg: dict):\n",
    "        \"\"\"Connect to one MCP server (stdio).\"\"\"\n",
    "        try:\n",
    "            params = StdioServerParameters(**server_cfg)\n",
    "            read, write = await self.exit_stack.enter_async_context(stdio_client(params))\n",
    "            session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
    "            await session.initialize()\n",
    "\n",
    "            self.sessions.append(session)\n",
    "\n",
    "            resp = await session.list_tools()\n",
    "            tools = resp.tools or []\n",
    "            print(f\"Connected to {server_name} with tools:\", [t.name for t in tools])\n",
    "\n",
    "            for t in tools:\n",
    "                self.tool_to_session[t.name] = session\n",
    "            self.available_tools.extend(self._openai_tools_from_mcp(tools))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self, config_path: str = \"server_config.json\"):\n",
    "        \"\"\"Read server_config.json and connect to each server.\"\"\"\n",
    "        if not os.path.exists(config_path):\n",
    "            print(f\"No {config_path} found — skipping MCP server connections.\")\n",
    "            return\n",
    "        try:\n",
    "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                cfg = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {config_path}: {e}\")\n",
    "            return\n",
    "\n",
    "        for name, server_cfg in (cfg.get(\"mcpServers\", {}) or {}).items():\n",
    "            await self.connect_to_server(name, server_cfg)\n",
    "\n",
    "    # ---------------------- Memory helpers (NEW) ----------------------\n",
    "    def _lc_history_to_openai_messages(self, history: Union[str, List[Any]]) -> List[Dict[str, str]]:\n",
    "        \"\"\"Convert LangChain memory history into Ollama/OpenAI-style messages.\"\"\"\n",
    "        msgs: List[Dict[str, str]] = []\n",
    "        # If ConversationSummaryBufferMemory returns a summary string\n",
    "        if isinstance(history, str):\n",
    "            if history.strip():\n",
    "                msgs.append({\"role\": \"system\", \"content\": f\"Conversation summary so far:\\n{history}\"})\n",
    "            return msgs\n",
    "\n",
    "        # Otherwise it's a list[BaseMessage]\n",
    "        for m in history:\n",
    "            if isinstance(m, HumanMessage):\n",
    "                msgs.append({\"role\": \"user\", \"content\": m.content})\n",
    "            elif isinstance(m, AIMessage):\n",
    "                msgs.append({\"role\": \"assistant\", \"content\": m.content})\n",
    "            elif isinstance(m, SystemMessage):\n",
    "                msgs.append({\"role\": \"system\", \"content\": m.content})\n",
    "            # Skip tool messages here; tool outputs are re-fed within the same turn\n",
    "        return msgs\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.history.clear()\n",
    "        print(\"✅ Memory cleared.\")\n",
    "\n",
    "    # ---------------------- Chat loop with memory ----------------------\n",
    "    async def process_query(self, query: str, model: Optional[str] = None):\n",
    "        model = model or self.model_default\n",
    "\n",
    "        # 1) Pull prior memory and convert to messages\n",
    "        prior = self.memory.load_memory_variables({}).get(\"history\", [])\n",
    "        memory_msgs = self._lc_history_to_openai_messages(prior)\n",
    "\n",
    "        # 2) Optional system preamble advertising tools\n",
    "        system_preamble = None\n",
    "        if self.available_tools:\n",
    "            tool_names = [t['function']['name'] for t in self.available_tools]\n",
    "            system_preamble = {\n",
    "                'role': 'system',\n",
    "                'content': 'You can call functions to satisfy user requests. Available tools: ' + ', '.join(tool_names)\n",
    "            }\n",
    "\n",
    "        # 3) Build the conversation to send to the model\n",
    "        messages: List[Dict[str, Any]] = []\n",
    "        messages.extend(memory_msgs)\n",
    "        if system_preamble:\n",
    "            messages.append(system_preamble)\n",
    "        messages.append({'role': 'user', 'content': query})\n",
    "\n",
    "        # 4) Full tool loop until model stops calling tools\n",
    "        final_text: str = \"\"\n",
    "        while True:\n",
    "            resp = self.client.chat_once(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                temperature=0.1,\n",
    "                tools=self.available_tools or None,\n",
    "                tool_choice='auto' if self.available_tools else 'none',\n",
    "            )\n",
    "            msg = resp['choices'][0]['message']\n",
    "            tool_calls = msg.get('tool_calls') or []\n",
    "\n",
    "            if tool_calls:\n",
    "                # Keep the assistant step (with tool_calls) in the transcript for the model\n",
    "                messages.append({\n",
    "                    'role': msg.get('role', 'assistant'),\n",
    "                    'content': msg.get('content', '') or \"\",\n",
    "                    'tool_calls': tool_calls,\n",
    "                })\n",
    "\n",
    "                for tc in tool_calls:\n",
    "                    fn = tc.get('function', {}) or {}\n",
    "                    name = fn.get('name')\n",
    "                    if not name:\n",
    "                        continue\n",
    "                    raw_args = fn.get('arguments') or '{}'\n",
    "                    try:\n",
    "                        args = json.loads(raw_args) if isinstance(raw_args, str) else (raw_args or {})\n",
    "                    except Exception:\n",
    "                        args = {}\n",
    "\n",
    "                    print(f\"Calling tool {name} with args {args}\")\n",
    "                    result_text = await self._call_mcp_tool(name, args)\n",
    "\n",
    "                    messages.append({\n",
    "                        'role': 'tool',\n",
    "                        'tool_call_id': tc.get('id') or name,  # Ollama may omit id\n",
    "                        'name': name,\n",
    "                        'content': result_text,\n",
    "                    })\n",
    "                # loop to let the model see tool outputs\n",
    "                continue\n",
    "\n",
    "            # No more tool calls — print final content if present and exit\n",
    "            final_text = (msg.get('content') or \"\").strip()\n",
    "            if final_text:\n",
    "                print(final_text)\n",
    "            break\n",
    "\n",
    "        # 5) Save this turn into memory (triggers summarization as needed)\n",
    "        try:\n",
    "            self.memory.save_context({\"input\": query}, {\"output\": final_text or \"[no text]\"})\n",
    "        except Exception as mem_err:\n",
    "            print(f\"(Memory error — continuing without memory this turn): {mem_err}\")\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        print(f\"\\nLocal MCP Chatbot (Ollama {self.model_default} + LangChain Memory) — type your query, 'forget' to clear memory, or 'quit' to exit.\")\n",
    "        while True:\n",
    "            try:\n",
    "                q = input(\"\\nQuery: \").strip()\n",
    "                if not q:\n",
    "                    continue\n",
    "                if q.lower() in (\"quit\", \"exit\"):\n",
    "                    break\n",
    "                if q.lower() == \"forget\":\n",
    "                    self.clear_memory()\n",
    "                    continue\n",
    "                await self.process_query(q)\n",
    "            except (EOFError, KeyboardInterrupt):\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    async def cleanup(self):\n",
    "        # Ensure all sessions and the stdio pipes are closed\n",
    "        try:\n",
    "            await self.exit_stack.aclose()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "async def main():\n",
    "    bot = MCP_ChatBot()\n",
    "    try:\n",
    "        await bot.connect_to_servers()  # discover and register tools\n",
    "        await bot.chat_loop()\n",
    "    finally:\n",
    "        await bot.cleanup()\n",
    "\n",
    "# Safe runner: works in normal terminals and in environments with an active event loop\n",
    "def _run_async(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        return asyncio.create_task(coro)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _run_async(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "05210c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! It's nice to meet you through the Ollama client. How can I assist you today? Whether you have questions, need information, or just want to chat, feel free to let me know!\n",
      "Sure, I can stream text for you! What would you like me to generate or discuss? Do you have any specific topic in mind?"
     ]
    }
   ],
   "source": [
    "from ollama import Client\n",
    "client = Client(host=\"http://127.0.0.1:11434\")\n",
    "\n",
    "# Non-streaming\n",
    "res = client.chat(model=\"qwen2.5:7b\", messages=[{\"role\":\"user\",\"content\":\"Hello from the Ollama client.\"}])\n",
    "print(res[\"message\"][\"content\"])\n",
    "\n",
    "# Streaming\n",
    "for part in client.chat(model=\"qwen2.5:7b\",\n",
    "                        messages=[{\"role\":\"user\",\"content\":\"Stream, please.\"}],\n",
    "                        stream=True):\n",
    "    print(part[\"message\"][\"content\"], end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4f33f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp-example",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
