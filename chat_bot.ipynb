{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7aaa823-c216-4d4f-837e-a88781e90912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mcp in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (1.12.4)\n",
      "Requirement already satisfied: anyio>=4.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (4.11.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (0.4.1)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (4.23.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (2.11.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.8.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (2.11.9)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (0.0.18)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (3.0.2)\n",
      "Requirement already satisfied: starlette>=0.27 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (0.46.2)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp) (0.29.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from anyio>=4.5->mcp) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from anyio>=4.5->mcp) (1.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from anyio>=4.5->mcp) (4.15.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.27->mcp) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.27->mcp) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->mcp) (0.14.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema>=4.20.0->mcp) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema>=4.20.0->mcp) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema>=4.20.0->mcp) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema>=4.20.0->mcp) (0.10.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.8.0->mcp) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.8.0->mcp) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.8.0->mcp) (0.4.1)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic-settings>=2.5.2->mcp) (0.21.0)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from uvicorn>=0.23.1->mcp) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn>=0.23.1->mcp) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: jupyter-server-proxy in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (4.4.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server-proxy) (3.10.5)\n",
      "Requirement already satisfied: jupyter-server>=1.24.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server-proxy) (2.14.1)\n",
      "Requirement already satisfied: simpervisor>=1.0.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server-proxy) (1.0.0)\n",
      "Requirement already satisfied: tornado>=6.1.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server-proxy) (6.4.1)\n",
      "Requirement already satisfied: traitlets>=5.1.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server-proxy) (5.14.3)\n",
      "Requirement already satisfied: anyio>=3.1.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (4.11.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (21.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (3.1.4)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (7.16.4)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (5.10.4)\n",
      "Requirement already satisfied: overrides>=5.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (7.4.0)\n",
      "Requirement already satisfied: packaging>=22.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (24.1)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.14.1)\n",
      "Requirement already satisfied: pywinpty>=2.0.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (2.0.10)\n",
      "Requirement already satisfied: pyzmq>=24 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-server>=1.24.0->jupyter-server-proxy) (1.8.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp->jupyter-server-proxy) (1.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.3.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from anyio>=3.1.0->jupyter-server>=1.24.0->jupyter-server-proxy) (4.15.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (21.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jinja2>=3.0.3->jupyter-server>=1.24.0->jupyter-server-proxy) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-client>=7.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy) (3.10.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server>=1.24.0->jupyter-server-proxy) (311)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (4.23.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (6.0.1)\n",
      "Requirement already satisfied: referencing in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (0.30.2)\n",
      "Requirement already satisfied: rfc3339-validator in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (1.5.0)\n",
      "Requirement already satisfied: pygments>=2.4.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.15.1)\n",
      "Requirement already satisfied: tinycss2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema>=2.15 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from nbformat>=5.3.0->jupyter-server>=1.24.0->jupyter-server-proxy) (2.16.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (1.16.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (0.5.1)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (2023.7.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema>=4.18.0->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (0.10.6)\n",
      "Requirement already satisfied: fqdn in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.5.1)\n",
      "Requirement already satisfied: isoduration in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (2.1)\n",
      "Requirement already satisfied: uri-template in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=24.6.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (24.11.1)\n",
      "Requirement already satisfied: cffi>=1.0.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (1.17.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server>=1.24.0->jupyter-server-proxy) (2.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server>=1.24.0->jupyter-server-proxy) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server>=1.24.0->jupyter-server-proxy) (1.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: openai in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (1.109.1)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai) (0.11.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai) (2.11.9)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from tqdm>4->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: python-dotenv in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (0.21.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: nest_asyncio in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.6.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from ollama) (2.11.9)\n",
      "Requirement already satisfied: anyio in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.27->ollama) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Downloading ollama-0.6.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install mcp\n",
    "%pip install jupyter-server-proxy\n",
    "%pip install openai\n",
    "%pip install python-dotenv\n",
    "%pip install nest_asyncio\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96af0c",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Create a .env file in project root with:\n",
    "\n",
    "OPENWEATHER_API_KEY=YOUR_KEY_HERE\n",
    "OPENWEATHER_BASE_URL=https://api.openweathermap.org/data/2.5/weather\n",
    "OPENAI_API_KEY=YOUR_OPENAI_KEY\n",
    "\n",
    "If only OPEN_AI_API_KEY exists it will be aliased automatically.\n",
    "Restart the kernel after changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a33bbe5-cc79-4e8a-ba37-4633a71a6780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # Load variables from .env if present\n",
    "\n",
    "def ensure_env(name: str, default: str | None = None, prompt: bool = True, secret: bool = False) -> str:\n",
    "    val = os.getenv(name)\n",
    "    if val:\n",
    "        return val\n",
    "    if default is not None:\n",
    "        os.environ[name] = default\n",
    "        return default\n",
    "    if prompt:\n",
    "        try:\n",
    "            entered = input(f\"Enter value for {name}: \").strip()\n",
    "            if entered:\n",
    "                os.environ[name] = entered\n",
    "                return entered\n",
    "        except Exception:\n",
    "            pass\n",
    "    raise RuntimeError(f\"{name} is not set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53d64e15-5aa2-4834-82b6-c7ad2fbf5bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str, unit: str = \"celsius\") -> Dict[str, Any]:\n",
    "    \"\"\"Fetch current weather from OpenWeather.\"\"\"\n",
    "    key = ensure_env(\"OPENWEATHER_API_KEY\", prompt=True)\n",
    "    base_url = os.getenv(\"OPENWEATHER_BASE_URL\") or \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "\n",
    "    unit_map = {\"celsius\": \"metric\", \"fahrenheit\": \"imperial\"}\n",
    "    owm_unit = unit_map.get(unit.lower(), \"metric\")\n",
    "    params = {\"q\": location, \"units\": owm_unit, \"appid\": key}\n",
    "    resp = requests.get(base_url, params=params, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    resolved_name = data.get(\"name\") or location\n",
    "    temp = (data.get(\"main\") or {}).get(\"temp\")\n",
    "    weather_list = data.get(\"weather\")\n",
    "    forecast = [w.get(\"description\") for w in weather_list if isinstance(w, dict) and w.get(\"description\")] if isinstance(weather_list, list) else []\n",
    "    return {\n",
    "        \"location\": resolved_name,\n",
    "        \"temperature\": temp,\n",
    "        \"unit\": \"celsius\" if owm_unit == \"metric\" else \"fahrenheit\",\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "# get_current_weather(\"Milan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9208723d-0851-4cc5-83ba-08b37eb0c72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI tool schemas (JSON Schema)\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get current weather for a location using OpenWeather. Units can be 'celsius' or 'fahrenheit'.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"City name (optionally with country code)\"},\n",
    "                    \"unit\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"default\": \"celsius\",\n",
    "                        \"description\": \"Temperature unit\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "                \"additionalProperties\": False\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c02a6245-f474-4549-acb3-8192304b7fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map tool names to Python callables\n",
    "mapping_tool_function = {\n",
    "    \"get_current_weather\": get_current_weather\n",
    "}\n",
    "\n",
    "def execute_tool(tool_name: str, tool_args: Dict[str, Any]) -> str:\n",
    "    result = mapping_tool_function[tool_name](**tool_args)\n",
    "    if result is None:\n",
    "        return \"The operation completed but didn't return any results.\"\n",
    "    if isinstance(result, list):\n",
    "        return \", \".join(map(str, result))\n",
    "    if isinstance(result, dict):\n",
    "        return json.dumps(result, indent=2)\n",
    "    return str(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb3e00ae-6aa9-46cb-9dec-11f8320f5e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using OpenAI model: gpt-3.5-turbo\n",
      "The current weather in New York is 21.05°C with light rain, and in Milan, it is 18.53°C with a clear sky.\n"
     ]
    }
   ],
   "source": [
    "# ----------------- OpenAI chat with tool calling -----------------\n",
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "BASE_OPENAI_URL = os.getenv(\"BASE_OPENAI_URL\", \"https://api.openai.com/v1\")\n",
    "client = OpenAI(api_key=api_key, base_url=BASE_OPENAI_URL)\n",
    "OPENAI_MODEL = \"gpt-3.5-turbo\"\n",
    "print(f\"Using OpenAI model: {OPENAI_MODEL}\")\n",
    "\n",
    "def process_query(query: str) -> None:\n",
    "    messages: List[Dict[str, Any]] = [{\"role\": \"user\", \"content\": query}]\n",
    "    response = client.chat.completions.create(\n",
    "        model=OPENAI_MODEL,\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",\n",
    "        temperature=0.01\n",
    "    )\n",
    "    while True:\n",
    "        choice = response.choices[0]\n",
    "        msg = choice.message\n",
    "        if msg.tool_calls:\n",
    "            messages.append({\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": msg.content or \"\",\n",
    "                \"tool_calls\": [tc.model_dump() for tc in msg.tool_calls]\n",
    "            })\n",
    "            for tc in msg.tool_calls:\n",
    "                name = tc.function.name\n",
    "                args = json.loads(tc.function.arguments or \"{}\")\n",
    "                result = execute_tool(name, args)\n",
    "                messages.append({\n",
    "                    \"role\": \"tool\",\n",
    "                    \"tool_call_id\": tc.id,\n",
    "                    \"content\": result\n",
    "                })\n",
    "            response = client.chat.completions.create(\n",
    "                model=OPENAI_MODEL,\n",
    "                messages=messages,\n",
    "                tools=tools,\n",
    "                tool_choice=\"auto\",\n",
    "                temperature=0.01\n",
    "            )\n",
    "            continue\n",
    "        if msg.content:\n",
    "            print(msg.content)\n",
    "        break\n",
    "    \n",
    "process_query(\"What's the weather like in New York and Milan?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecd4f94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"list\",\n",
      "  \"data\": [\n",
      "    {\n",
      "      \"id\": \"gpt-4-0613\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1686588896,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1687882411,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1677610602,\n",
      "      \"owned_by\": \"openai\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"sora-2-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759708663,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-mini-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759512137,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759517133,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-mini-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759517175,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"sora-2\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759708615,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"davinci-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692634301,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"babbage-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692634615,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-instruct\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1692901427,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-instruct-0914\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1694122472,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"dall-e-3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698785189,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"dall-e-2\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698798177,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-1106-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698957206,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1698959748,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-hd\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699046015,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699053241,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1-hd-1106\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1699053533,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-3-small\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1705948997,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-3-large\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1705953180,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-0125-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706037612,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706037777,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-0125\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1706048358,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1712361441,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4-turbo-2024-04-09\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1712601677,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1715367049,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-05-13\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1715368132,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-2024-07-18\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1721172717,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1721172741,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-08-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1722814719,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"chatgpt-4o-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1723515131,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-mini-2024-09-12\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1725648979,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1725649008,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2024-10-01\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727131766,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2024-10-01\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727389042,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727460443,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1727659998,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"omni-moderation-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1731689265,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"omni-moderation-2024-09-26\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1732734466,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1733945430,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734034239,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-realtime-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734112601,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-audio-preview-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734115920,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-2024-12-17\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734326976,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734375816,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-realtime-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734387380,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-audio-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1734387424,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1737146383,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-mini-2025-01-31\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1738010200,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-2024-11-20\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1739331543,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-search-preview-2025-03-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741388170,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-search-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741388720,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-search-preview-2025-03-11\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741390858,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-search-preview\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1741391161,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-transcribe\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742068463,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-transcribe\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742068596,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-pro-2025-03-19\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742251504,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o1-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742251791,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-mini-tts\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1742403959,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3-2025-04-16\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744133301,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-2025-04-16\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744133506,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o3\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744225308,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744225351,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744315746,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744316542,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-mini-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744317547,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744318173,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-nano-2025-04-14\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744321025,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4.1-nano\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1744321707,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-image-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1745517030,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"codex-mini-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1746673257,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-realtime-preview-2025-06-03\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748907838,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-4o-audio-preview-2025-06-03\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1748908498,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-deep-research\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1749685485,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"o4-mini-deep-research-2025-06-26\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1750866121,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-chat-latest\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754073306,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754075360,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425777,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-mini-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425867,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754425928,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-nano-2025-08-07\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754426303,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-nano\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1754426384,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-2025-08-28\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756256146,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756271701,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-realtime-2025-08-28\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756271773,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1756339249,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-codex\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1757527818,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-image-1-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1758845821,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-pro-2025-10-06\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759469707,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-5-pro\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759469822,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-audio-mini\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1759512027,\n",
      "      \"owned_by\": \"system\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"gpt-3.5-turbo-16k\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1683758102,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"tts-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1681940951,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"whisper-1\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1677532384,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"text-embedding-ada-002\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1671217299,\n",
      "      \"owned_by\": \"openai-internal\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6CmnzQZ:ckpt-step-95\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042481,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6ChQMTs:ckpt-step-95\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042148,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6ChRTvE:ckpt-step-190\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042149,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6ChRu7M\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042149,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6H0pg4f:ckpt-step-95\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726058728,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6H0qVBU:ckpt-step-190\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726058728,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6H0qNVD\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726058728,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6CmoRkI:ckpt-step-190\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042482,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"ft:gpt-3.5-turbo-0125:personal:my-test-model:A6CmoFtt\",\n",
      "      \"object\": \"model\",\n",
      "      \"created\": 1726042482,\n",
      "      \"owned_by\": \"user-hu2dhj7oyp1dpoznlrnrtyx9\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "import os\n",
    "!curl -s https://api.openai.com/v1/models -H \"Authorization: Bearer {os.environ['OPENAI_API_KEY']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1809b7-db8c-4779-aadd-c703083c01f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_loop():\n",
    "    print(\"Type your queries or 'quit' to exit.\")\n",
    "    while True:\n",
    "        try:\n",
    "            query = input(\"\\nQuery: \").strip()\n",
    "            if query.lower() == \"quit\":\n",
    "                break\n",
    "            process_query(query)\n",
    "            print()\n",
    "        except Exception as e:\n",
    "            print(f\"\\nError: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b02c22eb-60df-42c9-b3d2-8a179d007c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type your queries or 'quit' to exit.\n",
      "Hello! How can I assist you today?\n",
      "\n",
      "The current weather in Lecce, Provincia di Lecce is 18.89°C with broken clouds.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a79913c-aad4-4438-a103-751d011211f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting weather_server.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile weather_server.py\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "import os, json, requests\n",
    "from typing import Dict, Any\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "mcp = FastMCP(\"weather\")\n",
    "def ensure_env(name: str, default: str | None = None) -> str:\n",
    "    val = os.getenv(name)\n",
    "    if val:\n",
    "        return val\n",
    "    if default is not None:\n",
    "        os.environ[name] = default\n",
    "        return default\n",
    "    raise RuntimeError(f\"{name} is not set\")\n",
    "@mcp.tool()\n",
    "def get_current_weather(location: str, unit: str = \"celsius\") -> Dict[str, Any]:\n",
    "    key = ensure_env(\"OPENWEATHER_API_KEY\")\n",
    "    base_url = os.getenv(\"OPENWEATHER_BASE_URL\") or \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "    unit_map = {\"celsius\": \"metric\", \"fahrenheit\": \"imperial\"}\n",
    "    owm_unit = unit_map.get(unit.lower(), \"metric\")\n",
    "    params = {\"q\": location, \"units\": owm_unit, \"appid\": key}\n",
    "    resp = requests.get(base_url, params=params, timeout=15)\n",
    "    resp.raise_for_status()\n",
    "    data = resp.json()\n",
    "    resolved_name = data.get(\"name\") or location\n",
    "    temp = (data.get(\"main\") or {}).get(\"temp\")\n",
    "    weather_list = data.get(\"weather\")\n",
    "    forecast = [w.get(\"description\") for w in weather_list if isinstance(w, dict) and w.get(\"description\")] if isinstance(weather_list, list) else []\n",
    "    return {\n",
    "        \"location\": resolved_name,\n",
    "        \"temperature\": temp,\n",
    "        \"unit\": \"celsius\" if owm_unit == \"metric\" else \"fahrenheit\",\n",
    "        \"forecast\": forecast\n",
    "    }\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run(transport='stdio')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bd33fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_chatbot_openai.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_chatbot_openai.py\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, Any\n",
    "import asyncio, json, os, nest_asyncio\n",
    "load_dotenv()\n",
    "nest_asyncio.apply()\n",
    "\n",
    "def _flatten_tool_content(content_list):\n",
    "    parts = []\n",
    "    for item in content_list:\n",
    "        # TextContent (MCP) typically has .text\n",
    "        text = getattr(item, 'text', None)\n",
    "        if text is not None:\n",
    "            parts.append(text)\n",
    "        elif isinstance(item, dict):\n",
    "            parts.append(json.dumps(item, ensure_ascii=False))\n",
    "        else:\n",
    "            parts.append(str(item))\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        self.session: ClientSession | None = None\n",
    "        api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        self.available_tools: List[dict] = []\n",
    "\n",
    "    def _openai_tools_from_mcp(self, mcp_tools: List[types.Tool]) -> List[dict]:\n",
    "        out = []\n",
    "        for t in mcp_tools:\n",
    "            out.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description or \"\",\n",
    "                    \"parameters\": t.inputSchema or {\"type\": \"object\", \"properties\": {}}\n",
    "                }\n",
    "            })\n",
    "        return out\n",
    "\n",
    "    async def process_query(self, query: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        messages: List[Dict[str, Any]] = [{\"role\": \"user\", \"content\": query}]\n",
    "        while True:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=self.available_tools or None,\n",
    "                tool_choice=\"auto\" if self.available_tools else \"none\",\n",
    "                temperature=0.01\n",
    "            )\n",
    "            msg = response.choices[0].message\n",
    "            if msg.tool_calls:\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": msg.content or \"\",\n",
    "                    \"tool_calls\": [tc.model_dump() for tc in msg.tool_calls]\n",
    "                })\n",
    "                for tc in msg.tool_calls:\n",
    "                    tool_name = tc.function.name\n",
    "                    raw_args = tc.function.arguments\n",
    "                    try:\n",
    "                        args = json.loads(raw_args) if isinstance(raw_args, str) else (raw_args or {})\n",
    "                    except Exception:\n",
    "                        args = {}\n",
    "                    result = await self.session.call_tool(tool_name, arguments=args)\n",
    "                    flattened = _flatten_tool_content(result.content)\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tc.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": flattened\n",
    "                    })\n",
    "                continue\n",
    "            if msg.content:\n",
    "                print(msg.content.strip())\n",
    "            break\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        print(\"MCP Chatbot Started. Type your queries or 'quit'.\")\n",
    "        while True:\n",
    "            try:\n",
    "                q = input(\"\\nQuery: \").strip()\n",
    "                if q.lower() == 'quit':\n",
    "                    break\n",
    "                await self.process_query(q)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    async def connect_to_server_and_run(self):\n",
    "        server_params = StdioServerParameters(command=\"uv\", args=[\"run\", \"weather_server.py\"], env=None)\n",
    "        async with stdio_client(server_params) as (read, write):\n",
    "            async with ClientSession(read, write) as session:\n",
    "                self.session = session\n",
    "                await session.initialize()\n",
    "                resp = await session.list_tools()\n",
    "                self.available_tools = self._openai_tools_from_mcp(resp.tools)\n",
    "                await self.chat_loop()\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    await chatbot.connect_to_server_and_run()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53610722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict, Any\n",
    "from contextlib import AsyncExitStack\n",
    "import asyncio, json, os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "def _flatten_tool_content(content_list) -> str:\n",
    "    \"\"\"Flatten MCP CallToolResult.content into plain text for OpenAI 'tool' message.\"\"\"\n",
    "    parts = []\n",
    "    if isinstance(content_list, list):\n",
    "        for item in content_list:\n",
    "            # Most MCP content parts have a 'type' and either 'text' or 'data'\n",
    "            t = getattr(item, \"type\", None) or (isinstance(item, dict) and item.get(\"type\"))\n",
    "            if t == \"text\":\n",
    "                txt = getattr(item, \"text\", None) or (isinstance(item, dict) and item.get(\"text\"))\n",
    "                if txt is not None:\n",
    "                    parts.append(str(txt))\n",
    "            elif t in (\"json\", \"object\"):\n",
    "                data = getattr(item, \"data\", None) or (isinstance(item, dict) and (item.get(\"data\") or item.get(\"value\")))\n",
    "                try:\n",
    "                    parts.append(json.dumps(data, ensure_ascii=False))\n",
    "                except Exception:\n",
    "                    parts.append(str(data))\n",
    "            else:\n",
    "                try:\n",
    "                    parts.append(json.dumps(item, default=str, ensure_ascii=False))\n",
    "                except Exception:\n",
    "                    parts.append(str(item))\n",
    "    elif content_list is not None:\n",
    "        if isinstance(content_list, (dict, list)):\n",
    "            parts.append(json.dumps(content_list, ensure_ascii=False))\n",
    "        else:\n",
    "            parts.append(str(content_list))\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        self.client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.sessions: List[ClientSession] = []\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}\n",
    "        self.available_tools: List[ToolDefinition] = []  # OpenAI function-calling schema\n",
    "\n",
    "    def _openai_tools_from_mcp(self, mcp_tools: List[types.Tool]) -> List[dict]:\n",
    "        tools = []\n",
    "        for t in mcp_tools:\n",
    "            tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description or \"\",\n",
    "                    \"parameters\": t.inputSchema or {\"type\": \"object\", \"properties\": {}},\n",
    "                }\n",
    "            })\n",
    "        return tools\n",
    "\n",
    "    async def connect_to_server(self, server_name: str, server_cfg: dict):\n",
    "        \"\"\"Connect to one MCP server (stdio).\"\"\"\n",
    "        try:\n",
    "            params = StdioServerParameters(**server_cfg)\n",
    "            read, write = await self.exit_stack.enter_async_context(stdio_client(params))\n",
    "            session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
    "            await session.initialize()\n",
    "\n",
    "            # Remember the session\n",
    "            self.sessions.append(session)\n",
    "\n",
    "            # Discover tools for this server\n",
    "            resp = await session.list_tools()\n",
    "            tools = resp.tools or []\n",
    "            print(f\"Connected to {server_name} with tools:\", [t.name for t in tools])\n",
    "\n",
    "            # Map tool -> session and add to OpenAI tool list\n",
    "            for t in tools:\n",
    "                self.tool_to_session[t.name] = session\n",
    "            self.available_tools.extend(self._openai_tools_from_mcp(tools))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self, config_path: str = \"server_config.json\"):\n",
    "        \"\"\"Read server_config.json and connect to each server.\"\"\"\n",
    "        with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cfg = json.load(f)\n",
    "        servers = cfg.get(\"mcpServers\", {})\n",
    "        for name, server_cfg in servers.items():\n",
    "            await self.connect_to_server(name, server_cfg)\n",
    "\n",
    "    async def process_query(self, query: str, model: str = \"gpt-3.5-turbo\"):\n",
    "        messages: List[Dict[str, Any]] = [{\"role\": \"user\", \"content\": query}]\n",
    "\n",
    "        while True:\n",
    "            resp = self.client.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                tools=self.available_tools or None,\n",
    "                tool_choice=\"auto\" if self.available_tools else \"none\",\n",
    "                temperature=0.1,\n",
    "            )\n",
    "            msg = resp.choices[0].message\n",
    "\n",
    "            # Tool calls?\n",
    "            if msg.tool_calls:\n",
    "                # Keep assistant msg (with tool_calls) in history\n",
    "                messages.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": msg.content or \"\",\n",
    "                    \"tool_calls\": [tc.model_dump() for tc in msg.tool_calls],\n",
    "                })\n",
    "\n",
    "                for tc in msg.tool_calls:\n",
    "                    tool_name = tc.function.name\n",
    "                    raw_args = tc.function.arguments\n",
    "                    try:\n",
    "                        args = json.loads(raw_args) if isinstance(raw_args, str) else (raw_args or {})\n",
    "                    except Exception:\n",
    "                        args = {}\n",
    "\n",
    "                    session = self.tool_to_session.get(tool_name)\n",
    "                    if not session:\n",
    "                        # Unknown tool — tell the model\n",
    "                        messages.append({\n",
    "                            \"role\": \"tool\",\n",
    "                            \"tool_call_id\": tc.id,\n",
    "                            \"name\": tool_name,\n",
    "                            \"content\": f\"Tool '{tool_name}' is not available.\",\n",
    "                        })\n",
    "                        continue\n",
    "\n",
    "                    print(f\"Calling tool {tool_name} with args {args}\")\n",
    "                    result = await session.call_tool(tool_name, arguments=args)\n",
    "                    messages.append({\n",
    "                        \"role\": \"tool\",\n",
    "                        \"tool_call_id\": tc.id,\n",
    "                        \"name\": tool_name,\n",
    "                        \"content\": _flatten_tool_content(result.content),\n",
    "                    })\n",
    "\n",
    "                # Let the model read tool results and continue\n",
    "                continue\n",
    "\n",
    "            # Final answer (no tool calls)\n",
    "            if msg.content:\n",
    "                print(msg.content.strip())\n",
    "            break\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        print(\"\\nMCP Chatbot (OpenAI) — type your query, or 'quit' to exit.\")\n",
    "        while True:\n",
    "            try:\n",
    "                q = input(\"\\nQuery: \").strip()\n",
    "                if q.lower() == \"quit\":\n",
    "                    break\n",
    "                await self.process_query(q)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    async def run(self):\n",
    "        try:\n",
    "            await self.connect_to_servers()\n",
    "            await self.chat_loop()\n",
    "        finally:\n",
    "            await self.exit_stack.aclose()\n",
    "            \n",
    "    async def cleanup(self): # new\n",
    "        \"\"\"Cleanly close all resources using AsyncExitStack.\"\"\"\n",
    "        await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "\n",
    "async def main():\n",
    "    chatbot = MCP_ChatBot()\n",
    "    try:\n",
    "        # the mcp clients and sessions are not initialized using \"with\"\n",
    "        # like in the previous lesson\n",
    "        # so the cleanup should be manually handled\n",
    "        await chatbot.connect_to_servers() # new! \n",
    "        await chatbot.chat_loop()\n",
    "    finally:\n",
    "        await chatbot.cleanup() #new! \n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d36c372f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: litellm[proxy] in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (1.77.5)\n",
      "Requirement already satisfied: PyJWT<3.0.0,>=2.8.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (2.8.0)\n",
      "Requirement already satisfied: aiohttp>=3.10 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (3.10.5)\n",
      "Requirement already satisfied: apscheduler<4.0.0,>=3.10.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (3.11.0)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.15.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (1.25.0)\n",
      "Requirement already satisfied: azure-storage-blob<13.0.0,>=12.25.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (12.26.0)\n",
      "Requirement already satisfied: backoff in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (2.2.1)\n",
      "Requirement already satisfied: boto3==1.36.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (1.36.0)\n",
      "Requirement already satisfied: click in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (8.1.7)\n",
      "Requirement already satisfied: cryptography in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (43.0.0)\n",
      "Requirement already satisfied: fastapi<0.116.0,>=0.115.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.115.14)\n",
      "Requirement already satisfied: fastapi-sso<0.17.0,>=0.16.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.16.0)\n",
      "Requirement already satisfied: fastuuid>=0.13.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.13.5)\n",
      "Requirement already satisfied: gunicorn<24.0.0,>=23.0.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (23.0.0)\n",
      "Requirement already satisfied: httpx>=0.23.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.28.1)\n",
      "Requirement already satisfied: importlib-metadata>=6.8.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (7.0.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (3.1.4)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (4.23.0)\n",
      "Requirement already satisfied: litellm-enterprise==0.1.20 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.1.20)\n",
      "Requirement already satisfied: litellm-proxy-extras==0.2.22 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.2.22)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.10.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (1.12.4)\n",
      "Requirement already satisfied: openai>=1.99.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (1.109.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.7 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (3.11.3)\n",
      "Requirement already satisfied: polars<2.0.0,>=1.31.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (1.34.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (2.11.9)\n",
      "Requirement already satisfied: pynacl<2.0.0,>=1.5.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (1.5.0)\n",
      "Requirement already satisfied: python-dotenv>=0.2.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.21.0)\n",
      "Requirement already satisfied: python-multipart<0.0.19,>=0.0.18 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.0.18)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=6.0.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (6.0.1)\n",
      "Requirement already satisfied: rich==13.7.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (13.7.1)\n",
      "Requirement already satisfied: rq in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (2.6.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.11.0)\n",
      "Requirement already satisfied: tokenizers in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.22.1)\n",
      "Requirement already satisfied: uvicorn<0.30.0,>=0.29.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (0.29.0)\n",
      "Requirement already satisfied: websockets<14.0.0,>=13.1.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from litellm[proxy]) (13.1)\n",
      "Requirement already satisfied: botocore<1.37.0,>=1.36.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from boto3==1.36.0->litellm[proxy]) (1.36.26)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from boto3==1.36.0->litellm[proxy]) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.12.0,>=0.11.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from boto3==1.36.0->litellm[proxy]) (0.11.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from rich==13.7.1->litellm[proxy]) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from rich==13.7.1->litellm[proxy]) (2.15.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from aiohttp>=3.10->litellm[proxy]) (1.11.0)\n",
      "Requirement already satisfied: tzlocal>=3.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from apscheduler<4.0.0,>=3.10.4->litellm[proxy]) (5.3.1)\n",
      "Requirement already satisfied: azure-core>=1.31.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (1.35.1)\n",
      "Requirement already satisfied: msal>=1.30.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (1.34.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (4.15.0)\n",
      "Requirement already satisfied: isodate>=0.6.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from azure-storage-blob<13.0.0,>=12.25.1->litellm[proxy]) (0.7.2)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from cryptography->litellm[proxy]) (1.17.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from fastapi<0.116.0,>=0.115.5->litellm[proxy]) (0.46.2)\n",
      "Requirement already satisfied: oauthlib>=3.1.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from fastapi-sso<0.17.0,>=0.16.0->litellm[proxy]) (3.3.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from gunicorn<24.0.0,>=23.0.0->litellm[proxy]) (24.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.23.0->litellm[proxy]) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.23.0->litellm[proxy]) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.23.0->litellm[proxy]) (1.0.2)\n",
      "Requirement already satisfied: idna in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpx>=0.23.0->litellm[proxy]) (3.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx>=0.23.0->litellm[proxy]) (0.14.0)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from importlib-metadata>=6.8.0->litellm[proxy]) (3.17.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jinja2<4.0.0,>=3.1.2->litellm[proxy]) (2.1.3)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm[proxy]) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm[proxy]) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from jsonschema<5.0.0,>=4.22.0->litellm[proxy]) (0.10.6)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (0.4.1)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (2.11.0)\n",
      "Requirement already satisfied: pywin32>=310 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (311)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from mcp<2.0.0,>=1.10.0->litellm[proxy]) (3.0.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai>=1.99.5->litellm[proxy]) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai>=1.99.5->litellm[proxy]) (0.11.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai>=1.99.5->litellm[proxy]) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from openai>=1.99.5->litellm[proxy]) (4.66.5)\n",
      "Requirement already satisfied: polars-runtime-32==1.34.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from polars<2.0.0,>=1.31.0->litellm[proxy]) (1.34.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm[proxy]) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm[proxy]) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic<3.0.0,>=2.5.0->litellm[proxy]) (0.4.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from tiktoken>=0.7.0->litellm[proxy]) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from tiktoken>=0.7.0->litellm[proxy]) (2.32.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from click->litellm[proxy]) (0.4.6)\n",
      "Requirement already satisfied: croniter in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from rq->litellm[proxy]) (6.0.0)\n",
      "Requirement already satisfied: redis!=6,>=3.5 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from rq->litellm[proxy]) (6.4.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from tokenizers->litellm[proxy]) (0.35.3)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from azure-core>=1.31.0->azure-identity<2.0.0,>=1.15.0->litellm[proxy]) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from botocore<1.37.0,>=1.36.0->boto3==1.36.0->litellm[proxy]) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from botocore<1.37.0,>=1.36.0->boto3==1.36.0->litellm[proxy]) (2.2.3)\n",
      "Requirement already satisfied: pycparser in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography->litellm[proxy]) (2.21)\n",
      "Requirement already satisfied: filelock in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm[proxy]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm[proxy]) (2024.6.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich==13.7.1->litellm[proxy]) (0.1.0)\n",
      "Requirement already satisfied: email-validator>=2.0.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from pydantic[email]>=1.8.0->fastapi-sso<0.17.0,>=0.16.0->litellm[proxy]) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm[proxy]) (3.3.2)\n",
      "Requirement already satisfied: tzdata in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from tzlocal>=3.0->apscheduler<4.0.0,>=3.10.4->litellm[proxy]) (2023.3)\n",
      "Requirement already satisfied: pytz>2021.1 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from croniter->rq->litellm[proxy]) (2024.1)\n",
      "Requirement already satisfied: dnspython>=2.0.0 in c:\\users\\arsalan\\anaconda3\\lib\\site-packages (from email-validator>=2.0.0->pydantic[email]>=1.8.0->fastapi-sso<0.17.0,>=0.16.0->litellm[proxy]) (2.8.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install \"litellm[proxy]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1946ef77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mcp_chatbot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mcp_chatbot.py\n",
    "from dotenv import load_dotenv\n",
    "from mcp import ClientSession, StdioServerParameters, types\n",
    "from mcp.client.stdio import stdio_client\n",
    "from typing import List, Dict, TypedDict, Any, Optional\n",
    "from contextlib import AsyncExitStack\n",
    "import asyncio, json, os, requests, re, urllib.parse\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# ---------------------------\n",
    "# Ollama adapter (no OpenAI)\n",
    "# ---------------------------\n",
    "class OllamaAdapter:\n",
    "    def __init__(self, base: str = \"http://127.0.0.1:11434\"):\n",
    "        self.base = base.rstrip(\"/\")\n",
    "\n",
    "    def chat_once(self, model: str, messages: List[Dict[str, str]], temperature: float = 0.1):\n",
    "        payload = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\"temperature\": temperature},\n",
    "        }\n",
    "        r = requests.post(f\"{self.base}/api/chat\", json=payload, timeout=120)\n",
    "        r.raise_for_status()\n",
    "        data = r.json()\n",
    "        # Return an OpenAI-ish shape for minimal changes downstream\n",
    "        return {\n",
    "            \"choices\": [{\n",
    "                \"message\": {\n",
    "                    \"role\": data.get(\"message\", {}).get(\"role\", \"assistant\"),\n",
    "                    \"content\": data.get(\"message\", {}).get(\"content\", \"\"),\n",
    "                    \"tool_calls\": None,  # Ollama does not produce OpenAI-style tool_calls\n",
    "                }\n",
    "            }]\n",
    "        }\n",
    "\n",
    "\n",
    "def _flatten_tool_content(content_list) -> str:\n",
    "    \"\"\"Flatten MCP CallToolResult.content into plain text for display.\"\"\"\n",
    "    parts = []\n",
    "    if isinstance(content_list, list):\n",
    "        for item in content_list:\n",
    "            t = getattr(item, \"type\", None) or (isinstance(item, dict) and item.get(\"type\"))\n",
    "            if t == \"text\":\n",
    "                txt = getattr(item, \"text\", None) or (isinstance(item, dict) and item.get(\"text\"))\n",
    "                if txt is not None:\n",
    "                    parts.append(str(txt))\n",
    "            elif t in (\"json\", \"object\"):\n",
    "                data = (\n",
    "                    getattr(item, \"data\", None)\n",
    "                    or (isinstance(item, dict) and (item.get(\"data\") or item.get(\"value\")))\n",
    "                )\n",
    "                try:\n",
    "                    parts.append(json.dumps(data, ensure_ascii=False))\n",
    "                except Exception:\n",
    "                    parts.append(str(data))\n",
    "            else:\n",
    "                try:\n",
    "                    parts.append(json.dumps(item, default=str, ensure_ascii=False))\n",
    "                except Exception:\n",
    "                    parts.append(str(item))\n",
    "    elif content_list is not None:\n",
    "        if isinstance(content_list, (dict, list)):\n",
    "            parts.append(json.dumps(content_list, ensure_ascii=False))\n",
    "        else:\n",
    "            parts.append(str(content_list))\n",
    "    return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "class ToolDefinition(TypedDict):\n",
    "    name: str\n",
    "    description: str\n",
    "    input_schema: dict\n",
    "\n",
    "\n",
    "class MCP_ChatBot:\n",
    "    def __init__(self):\n",
    "        # Point directly at Ollama by default; override via env if needed\n",
    "        #   LOCAL_BASE_URL=http://127.0.0.1:11434\n",
    "        #   LOCAL_MODEL=qwen2.5:1.5b\n",
    "        self.model_default = os.getenv(\"LOCAL_MODEL\", \"qwen2.5:1.5b\")\n",
    "        self.client = OllamaAdapter(os.getenv(\"LOCAL_BASE_URL\", \"http://127.0.0.1:11434\"))\n",
    "\n",
    "        self.exit_stack = AsyncExitStack()\n",
    "        self.sessions: List[ClientSession] = []\n",
    "        self.tool_to_session: Dict[str, ClientSession] = {}\n",
    "        self.available_tools: List[ToolDefinition] = []  # schemas we discovered\n",
    "\n",
    "    # ---- MCP tooling helpers ----\n",
    "    def _openai_tools_from_mcp(self, mcp_tools: List[types.Tool]) -> List[dict]:\n",
    "        tools = []\n",
    "        for t in mcp_tools:\n",
    "            tools.append({\n",
    "                \"type\": \"function\",\n",
    "                \"function\": {\n",
    "                    \"name\": t.name,\n",
    "                    \"description\": t.description or \"\",\n",
    "                    \"parameters\": t.inputSchema or {\"type\": \"object\", \"properties\": {}},\n",
    "                }\n",
    "            })\n",
    "        return tools\n",
    "\n",
    "    def _tools_summary_text(self) -> str:\n",
    "        if not self.available_tools:\n",
    "            return \"No MCP tools are currently connected.\"\n",
    "        lines = []\n",
    "        for t in self.available_tools:\n",
    "            fn = t.get(\"function\", {})\n",
    "            name = fn.get(\"name\", \"unknown\")\n",
    "            desc = fn.get(\"description\", \"\") or \"\"\n",
    "            params = fn.get(\"parameters\", {}) or {}\n",
    "            lines.append(f\"- {name}: {desc} | params: {json.dumps(params, ensure_ascii=False)}\")\n",
    "        return \"\\n\".join(lines)\n",
    "\n",
    "    def _system_preamble(self) -> str:\n",
    "        # Keep the model from hallucinating about what MCP is.\n",
    "        summary = self._tools_summary_text()\n",
    "        return (\n",
    "            \"You are chatting in a local environment using a model served by Ollama. \"\n",
    "            \"MCP stands for Model Context Protocol. This runtime discovers MCP tools but \"\n",
    "            \"does not use OpenAI-style function calling automatically. \"\n",
    "            \"If the user asks about tools, show the list below.\\n\\n\"\n",
    "            \"Connected MCP tools:\\n\" + (summary if summary else \"None\")\n",
    "        )\n",
    "\n",
    "    # Convenience: call an MCP tool by name with JSON args\n",
    "    async def _call_mcp_tool(self, tool_name: str, args: dict) -> Optional[str]:\n",
    "        session = self.tool_to_session.get(tool_name)\n",
    "        if not session:\n",
    "            return None\n",
    "        try:\n",
    "            result = await session.call_tool(tool_name, arguments=args)\n",
    "            return _flatten_tool_content(result.content).strip()\n",
    "        except Exception as e:\n",
    "            return f\"Tool '{tool_name}' failed: {e}\"\n",
    "\n",
    "    # NEW: Let the model refine tool output into a nice human summary\n",
    "    async def _refine_with_model(\n",
    "        self,\n",
    "        tool_name: str,\n",
    "        tool_args: dict,\n",
    "        tool_output_text: str,\n",
    "        model: Optional[str] = None,\n",
    "    ) -> str:\n",
    "        \"\"\"Pass tool output to the LLM for natural-language refinement.\"\"\"\n",
    "        model = model or self.model_default\n",
    "\n",
    "        system = (\n",
    "            \"You are a precise assistant. You will be given FRESH data from a tool. \"\n",
    "            \"Write a concise, helpful answer for a general audience. \"\n",
    "            \"Do NOT invent numbers or facts; only use the tool output. \"\n",
    "            \"Prefer Celsius if unit=celsius, Fahrenheit if unit=fahrenheit. \"\n",
    "            \"If appropriate, add one short tip (e.g., umbrella/sunglasses) based strictly on conditions.\"\n",
    "        )\n",
    "\n",
    "        user = (\n",
    "            f\"Tool: {tool_name}\\n\"\n",
    "            f\"Args: {json.dumps(tool_args, ensure_ascii=False)}\\n\"\n",
    "            \"Raw tool output (JSON or text):\\n\"\n",
    "            \"```\\n\"\n",
    "            f\"{tool_output_text}\\n\"\n",
    "            \"```\\n\\n\"\n",
    "            \"Task: Summarize the current weather succinctly (1–3 sentences). \"\n",
    "            \"Include temperature with unit and the main condition. \"\n",
    "            \"If the tool data lacks a value, omit it rather than guessing.\"\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system},\n",
    "            {\"role\": \"user\", \"content\": user},\n",
    "        ]\n",
    "\n",
    "        resp = self.client.chat_once(model, messages, temperature=0.1)\n",
    "        msg = resp[\"choices\"][0][\"message\"]\n",
    "        return (msg.get(\"content\") or \"\").strip()\n",
    "\n",
    "    # ---- MCP discovery ----\n",
    "    async def connect_to_server(self, server_name: str, server_cfg: dict):\n",
    "        \"\"\"Connect to one MCP server (stdio).\"\"\"\n",
    "        try:\n",
    "            params = StdioServerParameters(**server_cfg)\n",
    "            read, write = await self.exit_stack.enter_async_context(stdio_client(params))\n",
    "            session = await self.exit_stack.enter_async_context(ClientSession(read, write))\n",
    "            await session.initialize()\n",
    "\n",
    "            # Remember the session\n",
    "            self.sessions.append(session)\n",
    "\n",
    "            # Discover tools for this server\n",
    "            resp = await session.list_tools()\n",
    "            tools = resp.tools or []\n",
    "            print(f\"Connected to {server_name} with tools:\", [t.name for t in tools])\n",
    "\n",
    "            # Map tool -> session and add to tool list\n",
    "            for t in tools:\n",
    "                self.tool_to_session[t.name] = session\n",
    "            self.available_tools.extend(self._openai_tools_from_mcp(tools))\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to connect to {server_name}: {e}\")\n",
    "\n",
    "    async def connect_to_servers(self, config_path: str = \"server_config.json\"):\n",
    "        \"\"\"Read server_config.json and connect to each server.\"\"\"\n",
    "        if not os.path.exists(config_path):\n",
    "            print(f\"No {config_path} found — skipping MCP server connections.\")\n",
    "            return\n",
    "        try:\n",
    "            with open(config_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                cfg = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {config_path}: {e}\")\n",
    "            return\n",
    "\n",
    "        servers = (cfg or {}).get(\"mcpServers\", {})\n",
    "        for name, server_cfg in servers.items():\n",
    "            await self.connect_to_server(name, server_cfg)\n",
    "\n",
    "        # After connecting, show a neat summary\n",
    "        print(\"\\n=== MCP tools summary ===\")\n",
    "        print(self._tools_summary_text())\n",
    "        print(\"=========================\\n\")\n",
    "\n",
    "    # ---- Intent routing helpers ----\n",
    "    _url_re = re.compile(r\"https?://\\S+\", re.I)\n",
    "\n",
    "    def _extract_url(self, text: str) -> Optional[str]:\n",
    "        m = self._url_re.search(text or \"\")\n",
    "        return m.group(0) if m else None\n",
    "\n",
    "    def _parse_weather(self, text: str) -> Optional[dict]:\n",
    "        \"\"\"\n",
    "        Naive weather intent parser.\n",
    "        Returns dict like {\"location\": \"Milan\", \"unit\": \"celsius\"} or None if not detected.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return None\n",
    "        lower = text.lower()\n",
    "        if \"weather\" not in lower:\n",
    "            return None\n",
    "\n",
    "        # try to extract location after \"in ...\"\n",
    "        loc = None\n",
    "        m = re.search(r\"\\b(?:in|at|for)\\s+([a-zA-Z\\u00C0-\\u017F\\s\\-']+)\", text)\n",
    "        if m:\n",
    "            loc = m.group(1).strip(\" .,!?:;\")\n",
    "\n",
    "        # Default to Celsius; switch if Fahrenheit mentioned\n",
    "        unit = \"celsius\"\n",
    "        if \"fahrenheit\" in lower or \"°f\" in lower:\n",
    "            unit = \"fahrenheit\"\n",
    "\n",
    "        # Also support prompts like \"Milan celsius\"\n",
    "        if not loc:\n",
    "            m2 = re.search(r\"\\b([A-Za-z\\u00C0-\\u017F][A-Za-z\\u00C0-\\u017F\\s\\-']+)\\s+(celsius|fahrenheit)\\b\", lower)\n",
    "            if m2:\n",
    "                loc = m2.group(1).strip()\n",
    "                unit = m2.group(2)\n",
    "\n",
    "        if not loc:\n",
    "            # last resort\n",
    "            m3 = re.search(r\"weather\\s+(?:in|at|for)?\\s*([A-Za-z\\u00C0-\\u017F][A-Za-z\\u00C0-\\u017F\\s\\-']*)\", lower)\n",
    "            if m3:\n",
    "                loc = m3.group(1).strip()\n",
    "\n",
    "        if not loc:\n",
    "            return None\n",
    "        return {\"location\": loc, \"unit\": unit}\n",
    "\n",
    "    # ---- Chat / routing ----\n",
    "    def _looks_like_tools_query(self, text: str) -> bool:\n",
    "        if not text:\n",
    "            return False\n",
    "        return bool(re.search(r\"\\b(mcp\\s+tools?|tools?|what.*tools|list.*tools)\\b\", text, re.I))\n",
    "\n",
    "    async def process_query(self, query: str, model: Optional[str] = None):\n",
    "        model = model or self.model_default\n",
    "\n",
    "        # 1) Deterministic tools listing\n",
    "        if self._looks_like_tools_query(query):\n",
    "            print(\"MCP tools available:\")\n",
    "            print(self._tools_summary_text())\n",
    "            return\n",
    "\n",
    "        # 2) Direct URL → use fetch tool if available\n",
    "        url = self._extract_url(query)\n",
    "        if url and \"fetch\" in self.tool_to_session:\n",
    "            print(f\"Fetching: {url}\")\n",
    "            content = await self._call_mcp_tool(\"fetch\", {\n",
    "                \"url\": url,\n",
    "                \"max_length\": 5000,\n",
    "                \"raw\": False\n",
    "            })\n",
    "            print(content or \"(no content)\")\n",
    "            return\n",
    "\n",
    "        # 3) Weather intent → call weather tool if present; else fallback to fetch wttr.in\n",
    "        w = self._parse_weather(query)\n",
    "        if w:\n",
    "            if \"get_current_weather\" in self.tool_to_session:\n",
    "                args = {\"location\": w[\"location\"], \"unit\": w[\"unit\"]}\n",
    "                print(f\"Calling MCP tool get_current_weather with {args}\")\n",
    "                content = await self._call_mcp_tool(\"get_current_weather\", args)\n",
    "\n",
    "                # Refine with the model for a human-friendly answer\n",
    "                refined = await self._refine_with_model(\n",
    "                    tool_name=\"get_current_weather\",\n",
    "                    tool_args=args,\n",
    "                    tool_output_text=content or \"\",\n",
    "                    model=model,\n",
    "                )\n",
    "                print(refined or (content or \"(no result)\"))\n",
    "                return\n",
    "\n",
    "            elif \"fetch\" in self.tool_to_session:\n",
    "                city = urllib.parse.quote(w[\"location\"])\n",
    "                url = f\"https://wttr.in/{city}?format=j1\"\n",
    "                print(f\"No get_current_weather tool. Falling back to fetch: {url}\")\n",
    "                raw = await self._call_mcp_tool(\"fetch\", {\"url\": url, \"raw\": True, \"max_length\": 10000})\n",
    "\n",
    "                # Create a compact JSON we pass to the model\n",
    "                compact = None\n",
    "                try:\n",
    "                    data = json.loads(raw) if isinstance(raw, str) else raw\n",
    "                    cur = (data.get(\"current_condition\") or [{}])[0]\n",
    "                    compact = {\n",
    "                        \"location\": w[\"location\"],\n",
    "                        \"unit\": \"celsius\",\n",
    "                        \"temperature\": cur.get(\"temp_C\"),\n",
    "                        \"feels_like\": cur.get(\"FeelsLikeC\"),\n",
    "                        \"desc\": (cur.get(\"weatherDesc\") or [{\"value\": \"\"}])[0].get(\"value\", \"\")\n",
    "                    }\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "                tool_text = json.dumps(compact, ensure_ascii=False) if compact else (raw or \"\")\n",
    "                refined = await self._refine_with_model(\n",
    "                    tool_name=\"fetch(wttr.in)\",\n",
    "                    tool_args={\"url\": url},\n",
    "                    tool_output_text=tool_text,\n",
    "                    model=model,\n",
    "                )\n",
    "                print(refined or (tool_text or \"(no result)\"))\n",
    "                return\n",
    "\n",
    "            else:\n",
    "                print(\"No weather-capable MCP tools connected (get_current_weather/fetch).\")\n",
    "                return\n",
    "\n",
    "        # 4) Normal chat (with preamble so the model knows it's offline for tools)\n",
    "        messages: List[Dict[str, Any]] = [\n",
    "            {\"role\": \"system\", \"content\": self._system_preamble()},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ]\n",
    "        resp = self.client.chat_once(model, messages, temperature=0.1)\n",
    "        msg = resp[\"choices\"][0][\"message\"]\n",
    "        if msg.get(\"content\"):\n",
    "            print(msg[\"content\"].strip())\n",
    "\n",
    "    async def chat_loop(self):\n",
    "        print(\"\\nMCP Chatbot (Ollama) — type your query, or 'quit' to exit.\")\n",
    "        print(f\"Using model: {self.model_default}\")\n",
    "        while True:\n",
    "            try:\n",
    "                q = input(\"\\nQuery: \").strip()\n",
    "                if q.lower() == \"quit\":\n",
    "                    break\n",
    "                await self.process_query(q)  # uses self.model_default\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "\n",
    "    async def run(self):\n",
    "        try:\n",
    "            await self.connect_to_servers()\n",
    "            await self.chat_loop()\n",
    "        finally:\n",
    "            await self.exit_stack.aclose()\n",
    "\n",
    "\n",
    "async def main():\n",
    "    bot = MCP_ChatBot()\n",
    "    await bot.run()\n",
    "\n",
    "\n",
    "# Safe runner: works in normal terminals and in environments with an active event loop\n",
    "def _run_async(coro):\n",
    "    try:\n",
    "        loop = asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.run(coro)\n",
    "    else:\n",
    "        return asyncio.create_task(coro)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    _run_async(main())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b2fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ollama\n",
    "from ollama import Client\n",
    "client = Client(host=\"http://127.0.0.1:11434\")\n",
    "\n",
    "# Non-streaming\n",
    "res = client.chat(model=\"deepseek-r1:1.5b\",\n",
    "                  messages=[{\"role\":\"user\",\"content\":\"Hello from the Ollama client.\"}])\n",
    "print(res[\"message\"][\"content\"])\n",
    "\n",
    "# Streaming\n",
    "for part in client.chat(model=\"deepseek-r1:1.5b\",\n",
    "                        messages=[{\"role\":\"user\",\"content\":\"Stream, please.\"}],\n",
    "                        stream=True):\n",
    "    print(part[\"message\"][\"content\"], end=\"\", flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
